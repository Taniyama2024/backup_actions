name: Daily Neon DB Backup to R2

on:
  schedule:
    - cron: "0 3 * * *" # ÊØéÊó•ÂçàÂâç3ÊôÇ(JST12:00)„Å´ÂÆüË°å
  workflow_dispatch: # ÊâãÂãïÂÆüË°å„ÇÇÂèØËÉΩ

jobs:
  backup:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check pg_dump version
        run: pg_dump --version

      - name: Install PostgreSQL client and AWS CLI (via pip)
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client python3-pip
          pip install awscli --upgrade --user
          echo "$HOME/.local/bin" >> $GITHUB_PATH
          aws --version

      - name: Dump Neon PostgreSQL database
        env:
          NEON_DB: ${{ secrets.NEON_DB }}
          NEON_HOST: ${{ secrets.NEON_HOST }}
          NEON_USER: ${{ secrets.NEON_USER }}
          NEON_PASSWORD: ${{ secrets.NEON_PASSWORD }}
        run: |
          set -e
          TIMESTAMP=$(date +"%Y-%m-%d_%H-%M-%S")
          FILE_NAME="backup_${TIMESTAMP}.sql.gz"
          echo "Creating backup file: $FILE_NAME"

          # ‚úÖ Êé•Á∂öURIÂΩ¢ÂºèÔºàSSL‰ªò„ÅçÔºâ
          PGPASSWORD=$NEON_PASSWORD pg_dump \
            "postgresql://$NEON_USER:$NEON_PASSWORD@$NEON_HOST:5432/$NEON_DB?sslmode=require" \
            | gzip > $FILE_NAME

          echo "Backup completed successfully: $FILE_NAME"
          echo "BACKUP_FILE=$FILE_NAME" >> $GITHUB_ENV

      - name: Upload backup to R2
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          set -e
          export AWS_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY
          export AWS_EC2_METADATA_DISABLED=true

          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          # R2„ÅÆbackups„Éï„Ç©„É´„ÉÄ„Å∏„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ
          aws s3 cp "$BACKUP_FILE" "s3://$R2_BUCKET/backups/$BACKUP_FILE" \
            --endpoint-url "$ENDPOINT"

          echo "‚úÖ Backup uploaded to R2: backups/$BACKUP_FILE"

      - name: Delete old backups (older than 30 days)
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          set -e
          export AWS_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY
          export AWS_EC2_METADATA_DISABLED=true

          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          echo "Checking for old backup files to delete (older than 30 days)..."

          aws s3 ls "s3://$R2_BUCKET/backups/" --endpoint-url "$ENDPOINT" | while read -r line; do
            CREATE_DATE=$(echo $line | awk '{print $1" "$2}')
            FILE_NAME=$(echo $line | awk '{print $4}')
            if [ -z "$FILE_NAME" ]; then
              continue
            fi

            FILE_DATE=$(date -d "$CREATE_DATE" +%s)
            NOW_DATE=$(date +%s)
            AGE_DAYS=$(( (NOW_DATE - FILE_DATE) / 86400 ))

            if [ $AGE_DAYS -gt 30 ]; then
              echo "üóëÔ∏è Deleting old backup: $FILE_NAME (age: $AGE_DAYS days)"
              aws s3 rm "s3://$R2_BUCKET/backups/$FILE_NAME" --endpoint-url "$ENDPOINT"
            fi
          done

          echo "Old backup cleanup completed."
