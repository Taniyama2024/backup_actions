name: Daily Neon DB Backup to R2

on:
  schedule:
    - cron: "0 3 * * *" # ÊØéÊó•ÂçàÂâç3ÊôÇ(JST12:00)„Å´ÂÆüË°å
  workflow_dispatch: # ÊâãÂãïÂÆüË°å„ÇÇÂèØËÉΩ

jobs:
  backup:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # --- PostgreSQL 17 client install ---
      - name: Install PostgreSQL 17 client
        run: |
          sudo apt-get update
          sudo apt-get install -y wget gnupg lsb-release
          echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" | sudo tee /etc/apt/sources.list.d/pgdg.list
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17
          pg_dump --version

      # --- pg_dump„Çí‰Ωø„Å£„Å¶Neon DB„Çí„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó ---
      - name: Dump Neon PostgreSQL database
        id: dump
        env:
          NEON_DB: ${{ secrets.NEON_DB }}
          NEON_HOST: ${{ secrets.NEON_HOST }}
          NEON_USER: ${{ secrets.NEON_USER }}
          NEON_PASSWORD: ${{ secrets.NEON_PASSWORD }}
        run: |
          set -euo pipefail

          TIMESTAMP=$(date +"%Y-%m-%d_%H-%M-%S")
          FILE_NAME="backup_${TIMESTAMP}.sql.gz"
          echo "Creating backup file: $FILE_NAME"

          # Neon DB„Å∏„ÉÄ„É≥„Éó
          PGPASSWORD=$NEON_PASSWORD pg_dump \
            --host="$NEON_HOST" \
            --port=5432 \
            --username="$NEON_USER" \
            --dbname="$NEON_DB" \
            --format=p \
            --compress=0 \
            --no-password \
          | gzip > "$FILE_NAME"

          # „Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫Á¢∫Ë™çÔºà0„Éê„Ç§„Éà„Å™„ÇâÂ§±ÊïóÊâ±„ÅÑÔºâ
          if [ ! -s "$FILE_NAME" ]; then
            echo "‚ùå Backup file is empty. Dump failed."
            exit 1
          fi

          echo "‚úÖ Backup completed successfully: $FILE_NAME"
          echo "BACKUP_FILE=$FILE_NAME" >> $GITHUB_ENV

      # --- AWS CLI v2 install ---
      - name: Install AWS CLI v2
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip -q awscliv2.zip
          sudo ./aws/install --update
          aws --version

      # --- R2„Å∏„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ ---
      - name: Upload backup to R2
        id: upload
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          set -e
          export AWS_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY
          export AWS_EC2_METADATA_DISABLED=true
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          aws s3 cp "$BACKUP_FILE" "s3://$R2_BUCKET/backups/$BACKUP_FILE" \
            --endpoint-url "$ENDPOINT" \
            --content-type "application/sql" \
            --quiet

          echo "‚úÖ Backup uploaded to R2: backups/$BACKUP_FILE"

      # --- Âè§„ÅÑ„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó„ÅÆÂâäÈô§Ôºà30Êó•‰ª•‰∏äÂâçÔºâ ---
      - name: Delete old backups (older than 30 days)
        id: cleanup
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          set -e
          export AWS_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY
          export AWS_EC2_METADATA_DISABLED=true
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          echo "üßπ Checking for old backup files to delete (older than 30 days)..."

          aws s3 ls "s3://$R2_BUCKET/backups/" --endpoint-url "$ENDPOINT" | while read -r line; do
            CREATE_DATE=$(echo "$line" | awk '{print $1" "$2}')
            FILE_NAME=$(echo "$line" | awk '{print $4}')
            if [ -z "$FILE_NAME" ]; then
              continue
            fi
            FILE_DATE=$(date -d "$CREATE_DATE" +%s)
            NOW_DATE=$(date +%s)
            AGE_DAYS=$(( (NOW_DATE - FILE_DATE) / 86400 ))

            if [ $AGE_DAYS -gt 30 ]; then
              echo "üóëÔ∏è Deleting old backup: $FILE_NAME (age: $AGE_DAYS days)"
              aws s3 rm "s3://$R2_BUCKET/backups/$FILE_NAME" --endpoint-url "$ENDPOINT"
            fi
          done

          echo "‚úÖ Old backup cleanup completed."

      # ------------------------
      # ÊàêÂäü„ÉªÂ§±Êïó„Å´Èñ¢„Çè„Çâ„ÅöÈÄöÁü•ÔºàGmail SMTPÔºâ
      # ------------------------
„ÄÄ„ÄÄ  - name: Send notification email (Gmail SMTP)
 „ÄÄ„ÄÄ„ÄÄ if: always()
„ÄÄ„ÄÄ„ÄÄ  uses: dawidd6/action-send-mail@v3
„ÄÄ„ÄÄ„ÄÄ„ÄÄwith:
    server_address: smtp.gmail.com
    server_port: 465
    username: ${{ secrets.ALERT_EMAIL_USERNAME }}
    password: ${{ secrets.ALERT_EMAIL_APP_PASSWORD }}
    subject: "Neon DB Backup ‚Äî ${{ github.repository }} run #${{ github.run_number }}"
    body: |
      Neon DB Backup workflow finished (run #${{ github.run_number }}).

      Repository: ${{ github.repository }}
      Workflow: ${{ github.workflow }}
      Run URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

      Summary of important steps:
      - dump step outcome:   ${{ steps.dump.outcome }}
      - upload step outcome: ${{ steps.upload.outcome }}
      - cleanup outcome:     ${{ steps.cleanup.outcome }}

      Backup file: ${{ env.BACKUP_FILE || 'N/A' }}

      Check full logs here: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

      ‚Äª„Åì„ÅÆ„É°„Éº„É´„ÅØËá™ÂãïÈÄÅ‰ø°„Åß„Åô„ÄÇ
    to: ${{ secrets.ALERT_EMAIL_RECIPIENTS }}
    from: ${{ secrets.ALERT_EMAIL_USERNAME }}
    secure: true
