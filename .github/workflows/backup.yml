name: Daily Neon DB Backup to R2

on:
  schedule:
    - cron: "0 3 * * *" # æ¯æ—¥åˆå‰3æ™‚(JST)
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install PostgreSQL 17 client
        run: |
          sudo apt-get update
          sudo apt-get install -y wget gnupg lsb-release
          echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" | sudo tee /etc/apt/sources.list.d/pgdg.list
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17
          pg_dump --version

      - name: Dump Neon PostgreSQL database
        id: dump
        env:
          NEON_DB: ${{ secrets.NEON_DB }}
          NEON_HOST: ${{ secrets.NEON_HOST }}
          NEON_USER: ${{ secrets.NEON_USER }}
          NEON_PASSWORD: ${{ secrets.NEON_PASSWORD }}
        run: |
          set -euo pipefail

          TIMESTAMP=$(date +"%Y-%m-%d_%H-%M-%S")
          FILE_NAME="backup_${TIMESTAMP}.sql.gz"
          echo "Creating backup file: $FILE_NAME"

          PGPASSWORD=$NEON_PASSWORD pg_dump \
            --host="$NEON_HOST" \
            --port=5432 \
            --username="$NEON_USER" \
            --dbname="$NEON_DB" \
            --format=p \
            --compress=0 \
            --no-password \
          | gzip > "$FILE_NAME"

          if [ ! -s "$FILE_NAME" ]; then
            echo "âŒ Backup file is empty. Dump failed." >&2
            exit 1
          fi

          echo "âœ… Backup completed successfully: $FILE_NAME"
          echo "BACKUP_FILE=$FILE_NAME" >> $GITHUB_ENV

      - name: Install AWS CLI v2
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip -q awscliv2.zip
          sudo ./aws/install --update
          aws --version

      - name: Upload backup to R2
        id: upload
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          set -e
          export AWS_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY
          export AWS_EC2_METADATA_DISABLED=true
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          aws s3 cp "$BACKUP_FILE" "s3://$R2_BUCKET/backups/$BACKUP_FILE" \
            --endpoint-url "$ENDPOINT" \
            --content-type "application/sql" \
            --quiet

          echo "âœ… Backup uploaded to R2: backups/$BACKUP_FILE"

      - name: Delete old backups (older than 30 days)
        id: cleanup
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          set -e
          export AWS_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY
          export AWS_EC2_METADATA_DISABLED=true
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          echo "ğŸ§¹ Checking for old backup files to delete (older than 30 days)..."

          aws s3 ls "s3://$R2_BUCKET/backups/" --endpoint-url "$ENDPOINT" | while read -r line; do
            CREATE_DATE=$(echo "$line" | awk '{print $1" "$2}')
            FILE_NAME=$(echo "$line" | awk '{print $4}')
            if [ -z "$FILE_NAME" ]; then
              continue
            fi
            FILE_DATE=$(date -d "$CREATE_DATE" +%s)
            NOW_DATE=$(date +%s)
            AGE_DAYS=$(( (NOW_DATE - FILE_DATE) / 86400 ))

            if [ $AGE_DAYS -gt 30 ]; then
              echo "ğŸ—‘ï¸ Deleting old backup: $FILE_NAME (age: $AGE_DAYS days)"
              aws s3 rm "s3://$R2_BUCKET/backups/$FILE_NAME" --endpoint-url "$ENDPOINT"
            fi
          done

          echo "âœ… Old backup cleanup completed."

      # é€šçŸ¥ç”¨ã«è¡¨ç¤ºã™ã‚‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—åã‚’æ•´ãˆã‚‹ï¼ˆå¤±æ•—æ™‚ã®ã¿ï¼‰
      - name: Prepare notification context (on failure)
        if: failure()
        run: |
          if [ -z "${BACKUP_FILE:-}" ]; then
            echo "NOTIFY_BACKUP_FILE=N/A" >> $GITHUB_ENV
          else
            echo "NOTIFY_BACKUP_FILE=$BACKUP_FILE" >> $GITHUB_ENV
          fi

      # å¤±æ•—æ™‚ã®ã¿ãƒ¡ãƒ¼ãƒ«é€ä¿¡
      - name: Send failure notification email (Gmail SMTP)
        if: failure()
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          username: ${{ secrets.ALERT_EMAIL_USERNAME }}
          password: ${{ secrets.ALERT_EMAIL_APP_PASSWORD }}
          subject: "âŒ Neon DB Backup FAILED â€” ${{ github.repository }} run #${{ github.run_number }}"
          body: |
            Neon DB Backup workflow failed (run #${{ github.run_number }}).

            Repository: ${{ github.repository }}
            Workflow: ${{ github.workflow }}
            Run URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

            Summary of important steps:
            - dump step outcome:   ${{ steps.dump.outcome }}
            - upload step outcome: ${{ steps.upload.outcome }}
            - cleanup outcome:     ${{ steps.cleanup.outcome }}

            Backup file (if any): ${{ env.NOTIFY_BACKUP_FILE }}

            Check full logs here: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

            â€»ã“ã®ãƒ¡ãƒ¼ãƒ«ã¯è‡ªå‹•é€ä¿¡ã§ã™ã€‚
          to: ${{ secrets.ALERT_EMAIL_RECIPIENTS }}
          from: ${{ secrets.ALERT_EMAIL_USERNAME }}
          secure: true
